{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2455a94",
   "metadata": {},
   "source": [
    "# Intro to Web Scraping with Python\n",
    "\n",
    "Welcome to Intro to Web Scraping with Python! This Tech Lab will cover the basics of web scraping, including getting the HTML for a page, parsing the HTML, and storing the results in a structured format. \n",
    "\n",
    "In this Tech Lab, you will scrape the locations of all of CRA's offices from its website. CRA's office locations can be found at [https://www.crai.com/locations/](https://www.crai.com/locations/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef42572e",
   "metadata": {},
   "source": [
    "To start, we'll import three packages. Those are:\n",
    " - requests, which is used for making HTTP requests and pulling the HTML from a website\n",
    " - Beautiful Soup (bs4), which is used to parse HTML and make it searchable and navigable\n",
    " - pandas, for storing the results of the web scrape in a tabular format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4edd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddd820a",
   "metadata": {},
   "source": [
    "### The Requests Package\n",
    "The [Requests](https://docs.python-requests.org/en/latest/) package in Python provides Python users with a way to make HTTP requests. Using the [`get`](https://docs.python-requests.org/en/latest/api/#requests.get) function, users can make HTTP GET requests. The function returns a [`Response`](https://docs.python-requests.org/en/latest/api/#requests.Response) object, which contains the HTTP response.\n",
    "\n",
    "For more information, see the [quickstart](https://docs.python-requests.org/en/latest/user/quickstart/) in the Requests \n",
    "documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1646d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.crai.com/locations/\"\n",
    "r = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94c30ae",
   "metadata": {},
   "source": [
    "In the above code, a request is made to the `\"www.crai.com/locations\"` URL, with the response stored in the variable `r`.\n",
    "\n",
    "`Response` objects have a `.text` attribute that can be used to access the text of a response. In this case, since a website was requested, the text is presumably the HTML of the website that was requested. See below for a snippet of `r.text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdef07dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r.text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1cbc64",
   "metadata": {},
   "source": [
    "Text returned as part of an HTTP response doesn't necisarily need to be HTML. For example, Web APIs return JSON (another text-based data type) via HTTP. However, we can verify that the text is HTML based on the `<!doctype html>` statement at the top of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e545b4de",
   "metadata": {},
   "source": [
    "### The Beautiful Soup Package\n",
    "\n",
    "With the HTML pulled into Python, that HTML can now be scraped! The Beautiful Soup package facilitates the scraping of HTML by parsing the text and making it searchable and navigable from within Python.\n",
    "\n",
    "To start using Beautiful Soup, simply covert the HTML into \"soup\" by using `bs4.BeautifulSoup`.\n",
    "\n",
    "*Note: The Beautiful Soup package is officially named bs4 (Beautiful Soup 4). The associated pip install command is therefore `pip install bs4`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1772d92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs4.BeautifulSoup(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868e19f0",
   "metadata": {},
   "source": [
    "We now have soup! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366ead94",
   "metadata": {},
   "source": [
    "Based on a brief analysis of CRA's \"Locations\" webpage, it looks like the name, address, country, and phone number of each CRA office is stored within a tag with class \"LocationCardListing__content\". We can use the [`find_all`](https://beautiful-soup-4.readthedocs.io/en/latest/index.html?highlight=find_all#find-all) function in Beautiful Soup to grab all such cards on the page. The `find_all` function looks through a tag's descendants (nested tags) and retrieves all descendants that match the given filters. \n",
    "\n",
    "Below we use the `find_all` function to search for tags with the class value \"LocationCardListing__content\". To do that, we pass the string \"LocationCardListing__content\" to the `class_` parameter of the `find_all` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916cb0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cards = soup.find_all(class_=\"LocationCardListing__content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893f192d",
   "metadata": {},
   "source": [
    "Let's take a look at what `find_all` returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12397b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "cards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d11d4a",
   "metadata": {},
   "source": [
    "`find_all` returns a list of all of the HTML tags that have class \"LocationCardListing__content\". While the result above just looks like HTML, we can inspect the returned list to get a little more information on the returned values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec2d358",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are {} location cards on the page.\".format(len(cards)))\n",
    "print()\n",
    "print(\"Each value is {}.\".format(type(cards[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f314348",
   "metadata": {},
   "source": [
    "The `find_all` function found 22 tags with class\"LocationCardListing__content. As we would expect from inspecting the website, each tag simply corresponds to a \"location card\" that contains the additional information on each the location.\n",
    "\n",
    "*Note: Different webpages are structured differently, so it's not always the case that the HTML of interest will have a class that uniquely identifies it. In those situations, filtering by tag name, id, or other HTML attribute may be appropriate. For more information, see \"Searching the Tree *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138f6387",
   "metadata": {},
   "source": [
    "The descendants (nested tags) of the HTML of each \"card\" can be accessed by using the `find\"` function. The `find` function can be called directly from a `Tag` element, and returns only the first instance of a descendant that matches the passed search criteria/filter. \n",
    "\n",
    "Below, the `find` function is used to pull the tag with class \"LocationCardListing__title\" from the first location card. Then, the `text` attribute of that tag is accessed to get just the associated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f413ae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "cards[0].find(class_=\"LocationCardListing__title\").text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6157d42c",
   "metadata": {},
   "source": [
    "The result above matches the CRA website, where \"Boston\" is the first office listed on the Locations page.\n",
    "\n",
    "We can now generalize the code above to loop through all location cards and loop through all of the four data points of interest - title, country, address, and phone number. Based on an inspection of the website, each of the four data points are uniquely identified by their associated class, so we can continue to use that to pull the information we want. \n",
    "\n",
    "Below, The text of each is pulled into a dictionary for each location, with the location-specific dictionaries then appended together into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b84cd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"LocationCardListing__country\", \"LocationCardListing__title\", \n",
    "            \"LocationCardListing__address\", \"LocationCardListing__phone\"]\n",
    "data = []\n",
    "for card in cards: #Loop through all location cards\n",
    "    card_data = {}\n",
    "    for c in classes: #For each class of interest (defined above)\n",
    "        tag = card.find(class_=c) #Find the tag with the associated class\n",
    "        if tag is not None: #If such a tag exists\n",
    "            cname = c.replace(\"LocationCardListing__\", \"\").title() \n",
    "            card_data[cname] = tag.text #Store the value/text associated with the tag\n",
    "    data.append(card_data) #Append all of the location card data to the 'data' variable\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e731cd",
   "metadata": {},
   "source": [
    "With the scraped data stored in a list of dictionaries, that data can be passed directly to pandas to be visualized as a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b3707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bbd272",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed521a80",
   "metadata": {},
   "source": [
    "### Now you try! \n",
    "Use what you've learned above to try to scrape the names from CRA's \"Our People\" page (just for employees whose last name begins with the letter \"A\"). For each employee on the page, scrape their name, title, office, phone number, and e-mail.\n",
    "\n",
    "As a reminder, a typical scraping structure would be:\n",
    " - Inspect the HTML of the webpage to identify how the webpage is structured.\n",
    " - Use requests to get the HTML of the page in Python\n",
    " - Parse the HTML using Beautiful Soup\n",
    " - Turn the data into tabular form using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e4452c",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_url = \"https://www.crai.com/our-people/?page=1&sort=role\"\n",
    "\n",
    "### (In your browser) inspect the HTML - No coding needed.\n",
    "\n",
    "\n",
    "### Request the HTML\n",
    "\n",
    "\n",
    "### Parse the data using Beautiful Soup\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Convert data into DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc23a28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
